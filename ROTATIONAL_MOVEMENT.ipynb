{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Dict, Box\n",
    "\n",
    "from Agents.agent import Agent\n",
    "from Constants.constants import WHITE, RED, BLUE, SCREEN_WIDTH, SCREEN_HEIGHT, LEVEL_5_WALLS\n",
    "from Walls.collision_detection import detect_collision\n",
    "from Walls.wall_class import Walls\n",
    "from Entities.turret import Turret\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LEVEL_5_WALLS = {\n",
    "    \"1\": {\"x\": 150, \"y\": 120, \"width\": 100, \"height\": 30},\n",
    "    \"2\": {\"x\": 300, \"y\": 80,  \"width\": 30,  \"height\": 60},\n",
    "    \"3\": {\"x\": 450, \"y\": 120, \"width\": 100, \"height\": 30},\n",
    "    \"4\": {\"x\": 600, \"y\": 180, \"width\": 30,  \"height\": 100},\n",
    "    \"5\": {\"x\": 500, \"y\": 400, \"width\": 100, \"height\": 30},\n",
    "    \"6\": {\"x\": 150, \"y\": 250, \"width\": 30,  \"height\": 200},\n",
    "    \"7\": {\"x\": 300, \"y\": 500, \"width\": 150, \"height\": 30}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 300}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super(GameEnv, self).__init__()\n",
    "\n",
    "        # defining the screen dimension for render purpose\n",
    "        self.screen_width = SCREEN_WIDTH\n",
    "        self.screen_height = SCREEN_HEIGHT\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.observation_space = Dict({\n",
    "            \"predator_position\": Box(low=np.array([0, 0], dtype=np.float32),\n",
    "                                     high=np.array([self.screen_width, self.screen_height], dtype=np.float32),\n",
    "                                     dtype=np.float32),\n",
    "\n",
    "            \"predator_angle\": Discrete(360),\n",
    "\n",
    "            \"bullet_position\": Box(low=np.array([0, 0], dtype=np.float32),\n",
    "                                   high=np.array([self.screen_width, self.screen_height], dtype=np.float32),\n",
    "                                   dtype=np.float32),\n",
    "\n",
    "            \"target_position\": Box(low=np.array([0, 0], dtype=np.float32),\n",
    "                                   high=np.array([self.screen_width, self.screen_height], dtype=np.float32),\n",
    "                                   dtype=np.float32),\n",
    "        })\n",
    "\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.predator_agent = Agent('predator', 0)\n",
    "        self.predator_total_reward = 0\n",
    "\n",
    "        self.obs = None\n",
    "\n",
    "        self.start_time = 0\n",
    "        self.animation_time = None\n",
    "        self.total_running_time = 10\n",
    "\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "        # for the wall initializations\n",
    "        self.wall = Walls(pygame)\n",
    "        self.walls = None\n",
    "\n",
    "        self.turret = Turret(SCREEN_WIDTH, SCREEN_HEIGHT)\n",
    "        self.bullet = self.turret.get_bullets()\n",
    "\n",
    "    def _get_obs(self):\n",
    "\n",
    "        if len(self.bullet) == 1:\n",
    "            bullet_pos = self.bullet[0].center\n",
    "        else:\n",
    "            bullet_pos = self.turret.center\n",
    "\n",
    "        observation = {\n",
    "            \"predator_position\": self.predator_agent.current_position,\n",
    "            \"predator_angle\": self.predator_agent.angle,\n",
    "            \"bullet_position\": bullet_pos,  # get bullet position\n",
    "            \"target_position\": self.turret.center,  # get the main target position\n",
    "        }\n",
    "        # print(f'observation:{observation}')\n",
    "        return observation\n",
    "\n",
    "    # def _get_info(self):\n",
    "    #     distance = 10000\n",
    "    #     self.goal_seen = is_ray_blocked(self.predator_agent.current_position, self.goal_coordinate, self.walls)\n",
    "    #     if self.goal_seen:\n",
    "    #         direction = self.goal_coordinate - self.predator_agent.current_position\n",
    "    #         distance = np.linalg.norm(direction)\n",
    "    #\n",
    "    #     info = {\n",
    "    #         \"goal_seen\": self.goal_seen,\n",
    "    #         \"distance\": distance,\n",
    "    #         \"vision_blocked\": not self.goal_seen,\n",
    "    #     }\n",
    "    #     # print(f'info: {info}')\n",
    "    #     return info\n",
    "\n",
    "    def get_reward(self, reward, done):\n",
    "        bullet_pos = 0\n",
    "        if len(self.bullet) == 1:\n",
    "            bullet_pos = self.bullet[0].pos\n",
    "        else:\n",
    "            bullet_pos = self.turret.position\n",
    "\n",
    "        if np.linalg.norm(np.abs(self.predator_agent.current_position - bullet_pos)) < self.predator_agent.radius + self.bullet[0].radius:\n",
    "            self.turret.destroy_bullet(self.bullet[0])\n",
    "            reward -= 50\n",
    "            done = True\n",
    "\n",
    "        if np.linalg.norm(np.abs(self.predator_agent.current_position - self.turret.position)) < self.predator_agent.radius + self.turret.radius:\n",
    "            reward += 100\n",
    "            done = True\n",
    "\n",
    "        reward += 0.01\n",
    "\n",
    "        distance_between_targets = np.linalg.norm(np.abs(self.predator_agent.current_position - self.turret.position))\n",
    "\n",
    "        reward += (1/distance_between_targets)\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "    def reset(self, seed=None, option=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        self.wall.clear_walls()\n",
    "        self.walls = self.wall.make_wall(LEVEL_5_WALLS)\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.predator_total_reward = 0\n",
    "        self.animation_time = time.time()\n",
    "\n",
    "        # for predator in self.predator_agents:\n",
    "        self.predator_agent.agent_reset(width=self.screen_width, height=self.screen_height)\n",
    "        # self.predator_agent.movement_speed = 300\n",
    "        self.turret.rotate_turret(self.predator_agent.center)\n",
    "\n",
    "        # all the variable values inside the observation space needs to be sent inside the observation variable\n",
    "        # for this level purpose we decided to add the dictionary observation\n",
    "        # set the observation to a dictionary\n",
    "        observation = self._get_obs()\n",
    "        # info = self._get_info()\n",
    "\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # initializing the return variables\n",
    "        done = False\n",
    "        reward = 0\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        current_time = time.time()\n",
    "\n",
    "        current_animation_time = time.time()\n",
    "        difference_in_animaton_time = current_animation_time - self.animation_time\n",
    "        self.animation_time = current_animation_time\n",
    "        # print(difference_in_animaton_time)\n",
    "        \n",
    "        elapsed_time = current_time - self.start_time\n",
    "\n",
    "        self.predator_agent.step_update(action, speed_factor=difference_in_animaton_time, range_x=self.screen_width, range_y=self.screen_height)\n",
    "        self.predator_agent = detect_collision(self.predator_agent, self.walls)\n",
    "\n",
    "        if len(self.turret.get_bullets()) == 0:\n",
    "            self.turret.shoot()\n",
    "\n",
    "        self.bullet[0].move(difference_in_animaton_time)\n",
    "        # if np.linalg.norm(np.abs(self.predator_agent.center - self.bullet[0].center)) < self.predator_agent.radius + self.bullet[0].radius:\n",
    "\n",
    "        # observation needs to be set a dictionary\n",
    "\n",
    "        self.total_steps += 1\n",
    "        reward, done = self.get_reward(reward, done)\n",
    "\n",
    "        if elapsed_time >= self.total_running_time:\n",
    "            reward -= 100\n",
    "            done = True\n",
    "\n",
    "        # getting observation and info\n",
    "        observation = self._get_obs()\n",
    "        # info = self._get_info()\n",
    "\n",
    "        self.predator_total_reward = reward\n",
    "        self.obs = observation\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, done, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'rgb_array':\n",
    "            self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "            pygame.font.init()\n",
    "\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        screen.fill((249, 245, 246))\n",
    "\n",
    "        # for the predator\n",
    "        predator = self.predator_agent\n",
    "        predator_rect = pygame.draw.circle(screen, (223, 106, 106), predator.center, predator.radius)\n",
    "        pygame.draw.line(screen, (223, 106, 106), predator.center, predator.draw_direction_end, 5)\n",
    "\n",
    "        # for turret\n",
    "        pygame.draw.circle(screen, (82, 82, 78), self.turret.center, self.turret.radius)\n",
    "        pygame.draw.line(screen, (82, 82, 78), self.turret.center, self.turret.rotate_turret(predator.center), 4)\n",
    "\n",
    "\n",
    "        # for the bullet\n",
    "        if len(self.bullet) != 0:\n",
    "            bullet_rect = pygame.draw.circle(screen, (115, 147, 167), self.bullet[0].center, self.bullet[0].radius)\n",
    "            self.turret.auto_destroy()\n",
    "            for wall in self.walls:\n",
    "                if bullet_rect.colliderect(wall):\n",
    "                    self.turret.destroy_bullet(self.bullet[0])\n",
    "            # if bullet_rect.collidelist(self.walls) or bullet_rect.colliderect(predator_rect):\n",
    "            #     self.turret.destroy_bullet(self.bullet[0])\n",
    "\n",
    "        for key, wall in LEVEL_5_WALLS.items():\n",
    "            pygame.draw.rect(screen, (71, 151, 177), (wall['x'], wall['y'], wall['width'], wall['height']))\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "\n",
    "            font = pygame.font.Font(None, 18)\n",
    "\n",
    "            text_surface = font.render(f\"Reward: {self.predator_total_reward: .5f} \", True, (0, 0, 0))\n",
    "\n",
    "            text_rect = text_surface.get_rect()\n",
    "\n",
    "            text_rect.center = (self.screen_width - 200, 10)\n",
    "\n",
    "            screen.blit(text_surface, text_rect)\n",
    "            self.window.blit(screen, screen.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # this part is to fix the fps of rendering\n",
    "            # self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "        else:\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.font.quit()\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "-48.47036157888918\n"
     ]
    }
   ],
   "source": [
    "env = GameEnv('human')\n",
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "print(env.total_steps)\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs', 'Final_1')\n",
    "model_path = os.path.join('Training', 'Models', 'Final_1')\n",
    "best_save_path = os.path.join('Training', 'Models', 'Final_1', 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv('human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=300, verbose=1)\n",
    "eval_callback = EvalCallback(env, callback_on_new_best=stop_callback, eval_freq=100000, best_model_save_path=best_save_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MultiInputPolicy', env, verbose=1, tensorboard_log=log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\Final_1\\DQN_1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Shanila\\CSE\\cse465\\New folder\\CSE_465\\test_test2.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/test_test2.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/test_test2.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m200000\u001b[39m, callback\u001b[39m=\u001b[39meval_callback)\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mlearn(\n\u001b[0;32m    268\u001b[0m         total_timesteps\u001b[39m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    269\u001b[0m         callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    270\u001b[0m         log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[0;32m    271\u001b[0m         tb_log_name\u001b[39m=\u001b[39mtb_log_name,\n\u001b[0;32m    272\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    273\u001b[0m         progress_bar\u001b[39m=\u001b[39mprogress_bar,\n\u001b[0;32m    274\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:312\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    309\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    311\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 312\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollect_rollouts(\n\u001b[0;32m    313\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv,\n\u001b[0;32m    314\u001b[0m         train_freq\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_freq,\n\u001b[0;32m    315\u001b[0m         action_noise\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_noise,\n\u001b[0;32m    316\u001b[0m         callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    317\u001b[0m         learning_starts\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_starts,\n\u001b[0;32m    318\u001b[0m         replay_buffer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer,\n\u001b[0;32m    319\u001b[0m         log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[0;32m    320\u001b[0m     )\n\u001b[0;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:544\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    541\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[0;32m    543\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n\u001b[0;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    547\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mstep(\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions[env_idx]\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "\u001b[1;32mc:\\Shanila\\CSE\\cse465\\New folder\\CSE_465\\test_test2.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/test_test2.ipynb#X13sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39m# print(difference_in_animaton_time)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/test_test2.ipynb#X13sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m elapsed_time \u001b[39m=\u001b[39m current_time \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/test_test2.ipynb#X13sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredator_agent\u001b[39m.\u001b[39mstep_update(action, speed_factor\u001b[39m=\u001b[39mdifference_in_animaton_time, range_x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscreen_width, range_y\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscreen_height)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/test_test2.ipynb#X13sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredator_agent \u001b[39m=\u001b[39m detect_collision(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredator_agent, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwalls)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/test_test2.ipynb#X13sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mturret\u001b[39m.\u001b[39mget_bullets()) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Shanila\\CSE\\cse465\\New folder\\CSE_465\\Agents\\agent.py:137\u001b[0m, in \u001b[0;36mAgent.step_update\u001b[1;34m(self, action, speed_factor, range_x, range_y)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_position \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_position \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdirection \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmovement_speed \u001b[39m*\u001b[39m speed_factor\n\u001b[0;32m    123\u001b[0m     \u001b[39m# self.get_direction()\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[0;32m    125\u001b[0m \u001b[39m# elif action == 3:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39m# elif action == 4:\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m#     pass\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_direction()\n\u001b[0;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_position[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_position[\u001b[39m0\u001b[39m], \u001b[39m10\u001b[39m, range_x\u001b[39m-\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_position[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_position[\u001b[39m1\u001b[39m], \u001b[39m10\u001b[39m, range_y\u001b[39m-\u001b[39m\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Shanila\\CSE\\cse465\\New folder\\CSE_465\\Agents\\agent.py:98\u001b[0m, in \u001b[0;36mAgent.get_direction\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdirection_end \u001b[39m=\u001b[39m directional_line_end\n\u001b[0;32m     97\u001b[0m direction \u001b[39m=\u001b[39m directional_line_end \u001b[39m-\u001b[39m center\n\u001b[1;32m---> 98\u001b[0m direction \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(direction)\n\u001b[0;32m     99\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdirection \u001b[39m=\u001b[39m direction\n\u001b[0;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdraw_direction_end \u001b[39m=\u001b[39m (center[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m directional_vector_x, center[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m directional_vector_y)\n",
      "File \u001b[1;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "model.learn(total_timesteps=200000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
